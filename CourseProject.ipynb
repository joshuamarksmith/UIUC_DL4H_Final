{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed3a4738-2e3b-4230-a760-5aa18887dbdd",
   "metadata": {},
   "source": [
    "# Deep Learning For Healthcare Course Project: INPREM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161bee9d-8169-434e-826e-2e4c9b29c687",
   "metadata": {},
   "source": [
    "https://www.kdd.org/kdd2020/accepted-papers/view/inprem-an-interpretable-and-trustworthy-predictive-model-for-healthcare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e2a581",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db47c1bd-157c-444a-be21-ebc3cdbd4645",
   "metadata": {},
   "source": [
    "## 1.1 Import Libraries\n",
    "\n",
    "We will need the [sparsemax](https://pypi.org/project/sparsemax/) library later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5063b31c-2e09-4c90-933d-34354f6fb5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: sparsemax in /opt/conda/lib/python3.8/site-packages (0.1.9)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.8/site-packages (from sparsemax) (1.11.0a0+bfe5ad2)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.8/site-packages (from torch->sparsemax) (4.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (5.9.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -U sparsemax\n",
    "!pip3 install -U psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3dc9044-b4f4-4504-99a1-041a4d89e6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psutil\n",
    "import pickle\n",
    "import json\n",
    "import ast\n",
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from icd9 import ICD9\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sparsemax import Sparsemax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dc863de-4dea-4fe9-b07c-f02da06a2872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no GPU found\n",
      "using demo data\n"
     ]
    }
   ],
   "source": [
    "# set seed\n",
    "seed = 24\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "# validate GPU usage\n",
    "print(\"using GPU\") if torch.cuda.is_available() else print(\"no GPU found\")\n",
    "\n",
    "# define data path\n",
    "use_demo = True\n",
    "if use_demo:\n",
    "    DATA_PATH = \"demodata/\" # work with open source data\n",
    "    print(\"using demo data\")\n",
    "else:\n",
    "    DATA_PATH = \"data/\" # work with certified patient data\n",
    "    print(\"using patient data\")\n",
    "\n",
    "# print metrics for each epoch (CPU, RAM, VRAM data) - will add time to each epoch\n",
    "print_metrics = False\n",
    "\n",
    "# icd9 tree structure\n",
    "tree = ICD9('codes.json')\n",
    "# tree.find('001.1').parent.parent.code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b317c7b-cdec-477b-ab90-f5004336588c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADMISSIONS.csv\t   D_ICD_DIAGNOSES.csv\trtypes.pkl  types.pkl\n",
      "DIAGNOSES_ICD.csv  ICUSTAYS.csv\t\trtypes.txt  types.txt\n"
     ]
    }
   ],
   "source": [
    "!ls {DATA_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59820ba-ebce-486b-8c10-013ffb6615cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.2 Import Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faafcd45-05f9-4855-89dc-c9f3afe2a3c9",
   "metadata": {},
   "source": [
    "First, we load the MIMIC-III dataset. We use 4 files from the dataset, `DIAGNOSES_ICD`, `D_ICD_DIAGNOSES`, `ICUSTAYS`, and `ADMISSIONS`. We initially format the data and drop the columns we don't need.\n",
    "\n",
    "For example, `subject_id` refers to a unique patient, `hadm_id` refers to a unique admission to the hospital, and `icustay_id` refers to a unique admission to an intensive care unit. We also keep the `out time` for each ICU stay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78d39bac-2632-4d00-9982-d3d5c915774d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diag_icd (1761 lines):\n",
      "    hadm_id icd9_code\n",
      "0   142345     99591\n",
      "1   142345     99662\n",
      "2   142345      5672\n",
      "3   142345     40391\n",
      "4   142345     42731\n",
      "\n",
      "icustays (136 lines):\n",
      "    subject_id  hadm_id  icustay_id              outtime\n",
      "0       10006   142345      206504  2164-10-25 12:21:07\n",
      "1       10011   105331      232110  2126-08-28 18:59:00\n",
      "2       10013   165520      264446  2125-10-07 15:13:52\n",
      "3       10017   199207      204881  2149-05-31 22:19:17\n",
      "4       10019   177759      228977  2163-05-16 03:47:04\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(filepath):\n",
    "    return pd.read_csv(filepath)\n",
    "\n",
    "def convert_datetime_to_day(df):\n",
    "    temp = pd.DataFrame()\n",
    "    temp[\"date\"] = pd.to_datetime(df['outtime'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    return str(temp['date'].dt.year) + str(temp['date'].dt.month) + str(temp['date'].dt.day)\n",
    "\n",
    "diag_icd = load_dataset(os.path.join(DATA_PATH, 'DIAGNOSES_ICD.csv'))\n",
    "icd_descriptions = load_dataset(os.path.join(DATA_PATH, 'D_ICD_DIAGNOSES.csv'))\n",
    "icustays = load_dataset(os.path.join(DATA_PATH, 'ICUSTAYS.csv'))\n",
    "admissions = load_dataset(os.path.join(DATA_PATH, 'ADMISSIONS.csv'))\n",
    "\n",
    "diag_icd = diag_icd.rename(columns={\"hadm_id\".upper(): \"hadm_id\", \"icd9_code\".upper(): \"icd9_code\"})\n",
    "icustays = icustays.rename(columns={\"subject_id\".upper(): \"subject_id\", \"hadm_id\".upper(): \"hadm_id\", \"icustay_id\".upper(): \"icustay_id\", \"outtime\".upper(): \"outtime\"})\n",
    "\n",
    "diag_icd = diag_icd[[\"hadm_id\", \"icd9_code\"]]\n",
    "icustays = icustays[[\"subject_id\", \"hadm_id\", \"icustay_id\", \"outtime\"]]\n",
    "\n",
    "\n",
    "print(f\"diag_icd ({len(diag_icd)} lines):\\n\", diag_icd.head(), end=\"\\n\\n\")\n",
    "print(f\"icustays ({len(icustays)} lines):\\n\", icustays.head(), end=\"\\n\\n\")\n",
    "# print(f\"admissions ({admissions.size} lines):\\n\", admissions.head(), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31023aa1",
   "metadata": {},
   "source": [
    "We merge the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b12ade38-be99-44c0-9fb7-e5c2dc2b1a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joined_df (1897 lines):\n",
      "   icd9_code  subject_id  icustay_id              outtime\n",
      "0     99591       10006      206504  2164-10-25 12:21:07\n",
      "1     99662       10006      206504  2164-10-25 12:21:07\n",
      "2      5672       10006      206504  2164-10-25 12:21:07\n",
      "3     40391       10006      206504  2164-10-25 12:21:07\n",
      "4     42731       10006      206504  2164-10-25 12:21:07\n"
     ]
    }
   ],
   "source": [
    "joined_df = pd.merge(diag_icd, icustays, how='inner', on='hadm_id')[[\"icd9_code\", \"subject_id\", \"icustay_id\", \"outtime\"]]\n",
    "print(f\"joined_df ({len(joined_df)} lines):\\n\", joined_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1f3438-5929-4f94-90fd-50e2af5b69b3",
   "metadata": {},
   "source": [
    "Clean up ICD-9 codes and convert to category labels for `y`, append all of our converted codes to `X` and `y`, and finally build a dictionary to convert types to codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c847451-714f-4bd3-b80a-a37a497105c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 19 patients\n"
     ]
    }
   ],
   "source": [
    "def convert_codes(codes):\n",
    "    \"\"\"Clean up ICD-9 codes and convert to category labels for `y`.\n",
    "    \n",
    "    If the codes contain 3, 4, or 5 digits, always use left 3. \n",
    "    If it starts with E, use (Exxx). If it starts with V, use (Vxx).\n",
    "    Also appends to a unique set of data to check number of unique codes.\n",
    "    \n",
    "    Inputs:\n",
    "        codes: a set of ICD-9 codes\n",
    "    \n",
    "    Outputs:\n",
    "        out: an array of category codes\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for code in codes:\n",
    "        code = str(code)\n",
    "        if code[0] == \"E\":\n",
    "            c = code[:4]\n",
    "        else:\n",
    "            c = code[:3]\n",
    "        out.append(c)\n",
    "        \n",
    "        unique_codes.add(c)\n",
    "       \n",
    "    return out\n",
    "\n",
    "\n",
    "def build_dictionaries(codes):\n",
    "    \"\"\"Construct dicts to map/ reverse map string input codes to keys, e.g. {'001': 0, '002': 1}\n",
    "    \n",
    "    Inputs:\n",
    "        codes: a set of unique category codes\n",
    "    \n",
    "    Outputs:\n",
    "        types: a dictionary to map codes to types\n",
    "        rtypes: a dictionary to map types to codes\n",
    "    \"\"\"\n",
    "    types = dict((diag, idx) for idx, diag in enumerate(codes))\n",
    "    rtypes = dict((idx, diag) for idx, diag in enumerate(codes))\n",
    "    \n",
    "    return types, rtypes\n",
    "\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "all_codes = []\n",
    "unique_codes = set()\n",
    "\n",
    "# set up X and y\n",
    "for name, patient in joined_df.sort_values(\"outtime\").groupby([\"subject_id\"]):\n",
    "    visits = []\n",
    "    for _, visit in patient.groupby([\"icustay_id\"]):\n",
    "        codes = visit[\"icd9_code\"].tolist()\n",
    "        codes = convert_codes(codes)\n",
    "        visits.append(codes)\n",
    "    if len(visits) >= 2:\n",
    "        x, y_ = visits[:-1], visits[-1]\n",
    "        X.append(x)\n",
    "        y.append(y_)\n",
    "\n",
    "types, rtypes = build_dictionaries(list(unique_codes))\n",
    "print(f\"Using {len(X)} patients\")\n",
    "\n",
    "# check mapping\n",
    "# print(unique_codes)\n",
    "# print('diag mapping for DIAG_V10:', types['V10']) # 75\n",
    "# print('reverse mapping for index 75:', rtypes[75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79702804-810c-41c6-9381-fb1681ac3cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(X) == len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69f80811-2ae9-44bf-81bc-77b08f9edabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275 codes\n",
      "275 types mappings\n"
     ]
    }
   ],
   "source": [
    "# print(\"visits (x):\", X, \"\\n\")\n",
    "# print(\"last visit (y):\", y)\n",
    "\n",
    "print(len(unique_codes), \"codes\") \n",
    "print(len(types), \"types mappings\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f406311-1f41-41f0-86dc-2d09a286533d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ICD 9 Codes for Binary Classification\n",
    "diabetes = (\"Diabetes\", \"250.xx\")\n",
    "heart_failure = (\"Heary Failure\", \"428.xx\")\n",
    "chronic_kidney_disease = (\"Chronic Kidney Disease\", \"585.xx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148ced3c",
   "metadata": {},
   "source": [
    "## 2. Build the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6eee9b2",
   "metadata": {},
   "source": [
    "## 2.1 Build Custom Dataset\n",
    "\n",
    "We implement a custom dataset using PyTorch class `Dataset`, which will characterize the key features of the dataset we want to generate.\n",
    "\n",
    "We will use the sequences of diagnosis codes seqs up to the last visit as input and the diagnosis code of last visit as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db4bab9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        '''Return the number of samples.\n",
    "        '''\n",
    "        \n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        '''Generates one sample of data.\n",
    "        '''\n",
    "        \n",
    "        return (self.x[index], self.y[index])\n",
    "        \n",
    "        \n",
    "dataset = CustomDataset(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f46fda0-2d1c-4529-a191-8f3f190e1b57",
   "metadata": {},
   "source": [
    "## 2.2 Collate Function\n",
    "\n",
    "This collate function `collate_fn()` will be called by `DataLoader` after fetching a list of samples using the indices from `CustomDataset` to collate the list of samples into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "784b31cb-ac66-46e2-80dd-b0d9813a973b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_retain(data):\n",
    "    \"\"\"\n",
    "    Collate the the list of samples into batches. For each patient, pad the diagnosis sequences \n",
    "    to the sample shape. The padding infomation is stored in `mask`.\n",
    "\n",
    "    Arguments:\n",
    "        data: a list of samples fetched from `CustomDataset`\n",
    "        \n",
    "    Outputs:\n",
    "        x: a tensor of shape (# patients, max # visits, max # diagnosis codes) of type torch.long\n",
    "        masks: a tensor of shape (# patients, max # visits, max # diagnosis codes) of type torch.bool\n",
    "        rev_x: same as x but in reversed time. This will be used in our RNN model for masking \n",
    "        rev_masks: same as mask but in reversed time. This will be used in our RNN model for masking\n",
    "        y: a tensor of shape (# patients) of type torch.float\n",
    "    \"\"\"\n",
    "    sequences, labels = zip(*data)\n",
    "    num_patients = len(sequences)\n",
    "    num_visits = [len(patient) for patient in sequences]\n",
    "    num_codes = [len(visit) for patient in sequences for visit in patient]\n",
    "\n",
    "    max_num_visits = max(num_visits)\n",
    "    max_num_codes = max(num_codes)\n",
    "    \n",
    "    y = torch.zeros((len(labels), len(types)), dtype=torch.bool)\n",
    "    \n",
    "    for i, label in enumerate(labels):\n",
    "        # create one-hot vector\n",
    "        for l in label:\n",
    "            plc = types[l]\n",
    "            y[i][plc] = True\n",
    "    \n",
    "    # y = y[y.any(axis=1)]\n",
    "    \n",
    "    x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    rev_x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "    rev_masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "    for i_patient, patient in enumerate(sequences):\n",
    "        count = 0\n",
    "        for j_visit, visit in enumerate(patient):\n",
    "            \"\"\"\n",
    "            TODO: update `x`, `rev_x`, `masks`, and `rev_masks`\n",
    "            \"\"\"\n",
    "            visit_len = len(visit)\n",
    "            \n",
    "            typed_visit = visit.copy()\n",
    "            for idx, code in enumerate(visit): # convert to mapping\n",
    "                typed_visit[idx] = types[visit[idx]]\n",
    "            \n",
    "            x[i_patient][j_visit][:visit_len] = torch.tensor(typed_visit, dtype=torch.long)\n",
    "            masks[i_patient][j_visit][:visit_len] = torch.ones((visit_len),dtype=torch.bool)\n",
    "            count += 1\n",
    "            \n",
    "        reverse_x = x[i_patient][:count]\n",
    "        reverse_mask = masks[i_patient][:count]\n",
    "        \n",
    "        rev_x[i_patient][:count] = torch.flip(reverse_x, [0])\n",
    "        rev_masks[i_patient][:count] = torch.flip(reverse_mask, [0])\n",
    "        \n",
    "    return x, masks, rev_x, rev_masks, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c81276f8-6ccc-4e9b-bc7d-bc16d43dbe82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275 14\n"
     ]
    }
   ],
   "source": [
    "num_visits = [len(patient) for patient in X]\n",
    "\n",
    "max_num_visits = max(num_visits)\n",
    "max_num_codes = len(types)\n",
    "\n",
    "def collate_fn_inprem(data):\n",
    "    \"\"\"\n",
    "    Collate the the list of samples into batches. For each patient, pad the diagnosis sequences \n",
    "    to the sample shape. \n",
    "        \n",
    "    Arguments:\n",
    "        data: a list of samples fetched from `CustomDataset`\n",
    "        \n",
    "    Outputs:\n",
    "        x: a tensor of shape (# patients, max # visits, max # diagnosis codes) of type torch.long\n",
    "        o:a tensor of shape (# patients, max # visits) of type torch.long\n",
    "        y: a tensor of shape (# patients) of type torch.float\n",
    "    \"\"\"\n",
    "    sequences, labels = zip(*data)\n",
    "    num_patients = len(sequences)\n",
    "\n",
    "    num_types = len(types)\n",
    "    max_num_codes = num_types\n",
    "    \n",
    "    y = torch.zeros((len(labels), num_types), dtype=torch.bool)\n",
    "    \n",
    "    for i, label in enumerate(labels):\n",
    "        # create one-hot vector\n",
    "        for l in label:\n",
    "            plc = types[l]\n",
    "            y[i][plc] = True\n",
    "        print(y[i])\n",
    "        \n",
    "    # y = y[y.any(axis=1)]\n",
    "    \n",
    "    x = torch.zeros((num_patients, max_num_visits, num_types), dtype=torch.long)\n",
    "    o = torch.zeros((num_patients, max_num_visits), dtype=torch.long)\n",
    "    masks = torch.zeros((num_patients, max_num_visits, num_types), dtype=torch.bool)\n",
    "    for i_patient, patient in enumerate(sequences):\n",
    "        count = 1\n",
    "        for j_visit, visit in enumerate(patient):\n",
    "            for code in visit : # convert to mapping\n",
    "                plc = types[code]\n",
    "                x[i_patient][j_visit][plc] = True\n",
    "            o[i_patient][j_visit] = count\n",
    "            count += 1\n",
    "    \n",
    "    return x, o, y\n",
    "\n",
    "\n",
    "print(max_num_codes, max_num_visits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c31b6a5",
   "metadata": {},
   "source": [
    "## 2.3 Split Dataset\n",
    "\n",
    "For each task, we randomly split each dataset into training and validation sets in an 80:20 ratio. This differs from the paper's implementation which uses a 75:10:15 training, testing, and validation split five times. We ran into issues evaluating the model against a test set and chose to return back to the 80:20 training/ validation split we had learned in previous implementations like RETAIN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0a20685d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset: 14\n",
      "Length of val dataset: 5\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "split = int(len(dataset)*0.75)\n",
    "\n",
    "lengths = [split, len(dataset) - split]\n",
    "train_dataset, val_dataset = random_split(dataset, lengths)\n",
    "\n",
    "print(\"Length of train dataset:\", len(train_dataset))\n",
    "print(\"Length of val dataset:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cf0295",
   "metadata": {},
   "source": [
    "## 2.4 Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c2d03a1e-7bd8-467c-b155-b1ab699ce8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def load_data(train_dataset, val_dataset, collate_fn):\n",
    "    '''    \n",
    "    Return the data loader for  train and validation dataset\n",
    "    \n",
    "    Arguments:\n",
    "        train dataset: train dataset of type `CustomDataset`\n",
    "        val dataset: validation dataset of type `CustomDataset`\n",
    "        collate_fn: collate function\n",
    "        \n",
    "    Outputs:\n",
    "        train_loader, val_loader: train and validation dataloaders\n",
    "    '''\n",
    "    \n",
    "    batch_size = 32\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn_inprem, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn_inprem, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "train_loader, val_loader= load_data(train_dataset, val_dataset, collate_fn_inprem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888c7b45-5f82-4852-8ad2-a7775c775d75",
   "metadata": {},
   "source": [
    "# 3. Build the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec381e0",
   "metadata": {},
   "source": [
    "We treat the medical events taking place in EHR as medical codes, which are denoted as  𝑐1,𝑐2,...𝑐|𝐶|  ∈ 𝐶, where |𝐶| is the total number of unique medical codes.\n",
    "\n",
    "One specific patient consist of a sequence of visits  𝑣1,𝑣2,...𝑣𝑇  where we denote the number of visits in total as T."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c92abe",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Retain (Baseline)\n",
    "\n",
    "First we implement the baseline RETAIN model used in the paper to compare against INPREM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7c4ac78d-f1f1-46bf-b466-05f0f8ab9bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_retain(train_dataset, val_dataset, collate_fn):\n",
    "    \n",
    "    batch_size = 32\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "train_loader_retain, val_loader_retain = load_data_retain(train_dataset, val_dataset, collate_fn_retain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c76ecafd-fa54-4c90-b355-77e91339af85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_sum(alpha, beta, rev_v, rev_masks):\n",
    "    \"\"\"\n",
    "    TODO: mask select the hidden states for true visits (not padding visits) and then\n",
    "        sum the them up.\n",
    "\n",
    "    Arguments:\n",
    "        alpha: the alpha attention weights of shape (batch_size, seq_length, 1)\n",
    "        beta: the beta attention weights of shape (batch_size, seq_length, hidden_dim)\n",
    "        rev_v: the visit embeddings in reversed time of shape (batch_size, # visits, embedding_dim)\n",
    "        rev_masks: the padding masks in reversed time of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "    Outputs:\n",
    "        c: the context vector of shape (batch_size, hidden_dim)\n",
    "        \n",
    "    NOTE: Do NOT use for loop.\n",
    "    \"\"\"\n",
    "    \n",
    "    out = None\n",
    "    \n",
    "    rev_masks = rev_masks.max(axis=2).values\n",
    "    rev_masks = rev_masks.unsqueeze(-1)\n",
    "    \n",
    "    v = rev_masks * rev_v\n",
    "    \n",
    "    out = torch.sum(alpha * beta * v, 1)\n",
    "    \n",
    "    return out\n",
    "\n",
    "def sum_embeddings_with_mask(x, masks):\n",
    "    \"\"\"\n",
    "    Mask select the embeddings for true visits (not padding visits) and then sum the embeddings for each visit up.\n",
    "\n",
    "    Arguments:\n",
    "        x: the embeddings of diagnosis sequence of shape (batch_size, # visits, # diagnosis codes, embedding_dim)\n",
    "        masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "    Outputs:\n",
    "        sum_embeddings: the sum of embeddings of shape (batch_size, # visits, embedding_dim)\n",
    "    \"\"\"\n",
    "    \n",
    "    x = x * masks.unsqueeze(-1)\n",
    "    x = torch.sum(x, dim = -2)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "eae07459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RETAIN(\n",
       "  (embedding): Embedding(275, 128)\n",
       "  (rnn_a): GRU(128, 128, batch_first=True)\n",
       "  (rnn_b): GRU(128, 128, batch_first=True)\n",
       "  (att_a): AlphaAttentionRetain(\n",
       "    (a_att): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       "  (att_b): BetaAttentionRetain(\n",
       "    (b_att): Linear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (fc): Linear(in_features=128, out_features=275, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alpha attention\n",
    "# beta attention\n",
    "# attention sum\n",
    "# sum embeddings with mask\n",
    "\n",
    "class AlphaAttentionRetain(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Define the linear layer `self.a_att` for alpha-attention using `nn.Linear()`;\n",
    "        \n",
    "        Arguments:\n",
    "            hidden_dim: the hidden dimension\n",
    "        \"\"\"\n",
    "        \n",
    "        self.a_att = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, g):\n",
    "        \"\"\"\n",
    "        TODO: Implement the alpha attention.\n",
    "        \n",
    "        Arguments:\n",
    "            g: the output tensor from RNN-alpha of shape (batch_size, seq_length, hidden_dim) \n",
    "        \n",
    "        Outputs:\n",
    "            alpha: the corresponding attention weights of shape (batch_size, seq_length, 1)\n",
    "            \n",
    "        HINT: consider `torch.softmax`\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        y = self.a_att(g)\n",
    "        y = torch.softmax(y, 2)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "class BetaAttentionRetain(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Define the linear layer `self.b_att` for beta-attention using `nn.Linear()`;\n",
    "        \n",
    "        Arguments:\n",
    "            hidden_dim: the hidden dimension\n",
    "        \"\"\"\n",
    "        \n",
    "        self.b_att = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "\n",
    "    def forward(self, h):\n",
    "        \"\"\"\n",
    "        TODO: Implement the beta attention.\n",
    "        \n",
    "        Arguments:\n",
    "            h: the output tensor from RNN-beta of shape (batch_size, seq_length, hidden_dim) \n",
    "        \n",
    "        Outputs:\n",
    "            beta: the corresponding attention weights of shape (batch_size, seq_length, hidden_dim)\n",
    "            \n",
    "        HINT: consider `torch.tanh`\n",
    "        \"\"\"\n",
    "        \n",
    "        y = self.b_att(h)\n",
    "        y = torch.tanh(y)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "\n",
    "class RETAIN(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_codes, embedding_dim=128):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_codes, embedding_dim)\n",
    "\n",
    "        self.rnn_a = nn.GRU(embedding_dim, embedding_dim, batch_first=True)\n",
    "        self.rnn_b = nn.GRU(embedding_dim, embedding_dim, batch_first=True)\n",
    "\n",
    "        self.att_a = AlphaAttentionRetain(embedding_dim)\n",
    "        self.att_b = BetaAttentionRetain(embedding_dim)\n",
    "\n",
    "        self.fc = nn.Linear(embedding_dim, num_codes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    \n",
    "    def forward(self, x, masks, rev_x, rev_masks):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            rev_x: the diagnosis sequence in reversed time of shape (# visits, batch_size, # diagnosis codes)\n",
    "            rev_masks: the padding masks in reversed time of shape (# visits, batch_size, # diagnosis codes)\n",
    "\n",
    "        Outputs:\n",
    "            probs: probabilities of shape (batch_size)\n",
    "        \"\"\"\n",
    "        rev_x = self.embedding(rev_x)\n",
    "        rev_x = sum_embeddings_with_mask(rev_x, rev_masks)\n",
    "\n",
    "        g, _ = self.rnn_a(rev_x)\n",
    "        h, _ = self.rnn_b(rev_x)\n",
    "\n",
    "        alpha = self.att_a(g)\n",
    "        beta = self.att_b(h)\n",
    "\n",
    "        c = attention_sum(alpha, beta, rev_x, rev_masks)\n",
    "\n",
    "        logits = self.fc(c)\n",
    "        probs = self.sigmoid(logits)\n",
    "        \n",
    "        return probs.squeeze()\n",
    "    \n",
    "    \n",
    "# load the model\n",
    "retain = RETAIN(num_codes = len(types))\n",
    "retain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "87d182e1-2f5b-41db-b617-5ea9e376b302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training Loss: 0.905809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to numpy.ndarray.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1027/2579131190.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m \u001b[0mtrain_retain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader_retain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader_retain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_1027/2579131190.py\u001b[0m in \u001b[0;36mtrain_retain\u001b[0;34m(model, train_loader, val_loader, n_epochs)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch: {} \\t Training Loss: {:.6f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroc_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_retain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch: {} \\t Validation p: {:.2f}, r:{:.2f}, f: {:.2f}, roc_auc: {:.2f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroc_auc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroc_auc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported format string passed to numpy.ndarray.__format__"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "\n",
    "def eval_retain(model, val_loader):\n",
    "    \n",
    "    \"\"\"\n",
    "    Evaluate the model.\n",
    "    \n",
    "    Arguments:\n",
    "        model: the RNN model\n",
    "        val_loader: validation dataloader\n",
    "        \n",
    "    Outputs:\n",
    "        precision: overall precision score\n",
    "        recall: overall recall score\n",
    "        f1: overall f1 score\n",
    "        roc_auc: overall roc_auc score\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    y_pred = torch.LongTensor()\n",
    "    y_score = torch.Tensor()\n",
    "    y_true = torch.LongTensor()\n",
    "    model.eval()\n",
    "    for x, masks, rev_x, rev_masks, y in val_loader:\n",
    "        y_logit = model(x, masks, rev_x, rev_masks)\n",
    "        y_hat = (y_logit > 0.5).int()\n",
    "        y_score = torch.cat((y_score,  y_logit.detach().to('cpu')), dim=0)\n",
    "        y_pred = torch.cat((y_pred,  y_hat.detach().to('cpu')), dim=0)\n",
    "        y_true = torch.cat((y_true, y.detach().to('cpu')), dim=0)\n",
    "    \n",
    "    p, r, f, _ = precision_recall_fscore_support(y_true, y_pred)\n",
    "    # roc_auc = roc_auc_score(y_true, y_score)\n",
    "    roc_auc = 0.0\n",
    "    \n",
    "    return p, r, f, roc_auc\n",
    "\n",
    "\n",
    "def train_retain(model, train_loader, val_loader, n_epochs):\n",
    "    \"\"\"\n",
    "    Train the model.\n",
    "    \n",
    "    Arguments:\n",
    "        model: the RNN model\n",
    "        train_loader: training dataloder\n",
    "        val_loader: validation dataloader\n",
    "        n_epochs: total number of epochs\n",
    "    \"\"\"\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        train_loss = 0\n",
    "        for x, masks, rev_x, rev_masks, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(x, masks, rev_x, rev_masks)\n",
    "            y = y.type(torch.FloatTensor)\n",
    "            loss = criterion(y_hat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()  \n",
    "            \n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        print('Epoch: {} \\t Training Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "        p, r, f, roc_auc = eval_retain(model, val_loader)\n",
    "        print('Epoch: {} \\t Validation p: {:.2f}, r:{:.2f}, f: {:.2f}, roc_auc: {:.2f}'.format(epoch+1, p, r, f, roc_auc))\n",
    "    \n",
    "    return round(roc_auc, 2)\n",
    "\n",
    "\n",
    "# load the model\n",
    "retain = RETAIN(num_codes = len(types))\n",
    "\n",
    "# load the loss function\n",
    "criterion = nn.BCELoss()\n",
    "# load the optimizer\n",
    "optimizer = torch.optim.Adam(retain.parameters(), lr=1e-3)\n",
    "\n",
    "n_epochs = 5\n",
    "train_retain(retain, train_loader_retain, val_loader_retain, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ce4102",
   "metadata": {},
   "source": [
    "## INPREM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ee0171-82a3-4f49-8baa-fef7e1f5efee",
   "metadata": {},
   "source": [
    "Each visit contains a subset of medical codes, and we denote each visit as a binary vector  $v_{t} ∈ \\{0, 1\\}_{|C|}$, where the 𝑖-th element is set to 1 if the 𝑡-th visit contains the medical code $c_{i}$, otherwise 0. The visits  $v_{1}, v_{2},... v_{T}$ are stacked to form an input matrix $X ∈ \\{0, 1\\}^{|C|xT}$ , which we use as the input for the network\n",
    "\n",
    "$E_{v} = {W}_{v}X$\n",
    "\n",
    "$E_{o} = {W}_{o}O$\n",
    "\n",
    "$E_{r} = \\alpha(\\beta \\odot (E_{v}+E_{o}))^{T}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "17b78ac0-1bc5-46fa-98ac-83ca4cd3695c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lin = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        self.sparsemax = Sparsemax()\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, g):\n",
    "        \n",
    "        y = self.lin(g)\n",
    "        sparse_max = self.sparsemax(y)\n",
    "        soft_max = F.softmax(y, dim=1)\n",
    "        \n",
    "        out = (sparse_max + soft_max) / 2\n",
    "        \n",
    "        return out\n",
    "    \n",
    "class BetaAttention(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lin = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, h):\n",
    "        \n",
    "        y = self.lin(h)\n",
    "        out = torch.tanh(y)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, num_codes, hidden_dim=256, num_attention_layers=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc_q = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_v = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_k = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        self.fc_multi = nn.Linear(num_attention_layers * hidden_dim, hidden_dim)\n",
    "        \n",
    "        self.conv1 = torch.nn.Conv1d(hidden_dim, hidden_dim, 1)\n",
    "        \n",
    "        self.conv2 = torch.nn.Conv1d(hidden_dim, hidden_dim, 1)\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, E):\n",
    "        \n",
    "        Q = self.fc_q(E)\n",
    "        K = self.fc_k(E)\n",
    "        V = self.fc_v(E)\n",
    "        \n",
    "        attn = V * F.softmax((Q * K) / math.sqrt(self.hidden_dim), dim=1)\n",
    "        \n",
    "        x = self.fc_multi(torch.concat((attn, attn), dim=2))\n",
    "        \n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "74799646-67a2-4780-a4e2-8eabedce15f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INPREM(\n",
       "  (embedding_v): Linear(in_features=275, out_features=256, bias=False)\n",
       "  (embedding_o): Linear(in_features=1, out_features=256, bias=False)\n",
       "  (att_a): AlphaAttention(\n",
       "    (lin): Linear(in_features=256, out_features=1, bias=True)\n",
       "    (sparsemax): Sparsemax(dim=-1)\n",
       "    (softmax): Softmax(dim=1)\n",
       "  )\n",
       "  (att_b): BetaAttention(\n",
       "    (lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (multi_attn): MultiHeadAttention(\n",
       "    (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (fc_multi): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (conv1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "    (conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (fc): Linear(in_features=256, out_features=275, bias=True)\n",
       "  (do): Dropout(p=0.5, inplace=False)\n",
       "  (tan): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class INPREM(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_codes, num_visits, embedding_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # self.embedding_v = nn.Embedding(num_codes, embedding_dim)\n",
    "        # self.embedding_o = nn.Embedding(num_codes, embedding_dim)\n",
    "        \n",
    "        self.embedding_v = nn.Linear(num_codes, embedding_dim, bias=False)\n",
    "        self.embedding_o = nn.Linear(1, embedding_dim, bias=False)\n",
    "        \n",
    "        self.att_a = AlphaAttention(embedding_dim)\n",
    "        self.att_b = BetaAttention(embedding_dim)\n",
    "        \n",
    "        self.multi_attn = MultiHeadAttention(num_codes, embedding_dim)\n",
    "        \n",
    "        self.fc = nn.Linear(embedding_dim, num_codes)\n",
    "        \n",
    "        self.do = nn.Dropout(.5)\n",
    "        \n",
    "        self.tan = nn.Tanh()\n",
    "    \n",
    "    def forward(self, X, O):\n",
    "        \"\"\"\n",
    "        Implement the INPREM Model\n",
    "        \"\"\"\n",
    "        \n",
    "        X = X.type(torch.FloatTensor)\n",
    "        O = O.type(torch.FloatTensor)\n",
    "        \n",
    "        O = O.unsqueeze(dim=2)\n",
    "        \n",
    "        # Get E_v\n",
    "        E_v = self.embedding_v(X)\n",
    "        E_o = self.embedding_o(O)\n",
    "        \n",
    "        E = torch.add(E_o,E_v)\n",
    "        \n",
    "        H =  self.multi_attn(E)\n",
    "        H =  self.multi_attn(H)\n",
    "        \n",
    "        beta = self.att_b(H)\n",
    "        alpha = self.att_a(H).permute(0,2,1)\n",
    "        \n",
    "        E_r = (alpha @ (beta * E))\n",
    "        \n",
    "        out = self.fc(E_r)\n",
    "        \n",
    "        out = out.squeeze()\n",
    "        out = F.softmax(out, dim=1)\n",
    "        # out = self.tan(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "# load the model here\n",
    "model = INPREM(len(types), max_num_visits)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3484ac7a-ed31-42ab-bac9-687b93486438",
   "metadata": {},
   "source": [
    "### INPREM_O (Removing Ordering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1893af8c-6609-4262-9351-b2af7de16ec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INPREM_O(\n",
       "  (embedding_v): Linear(in_features=1071, out_features=256, bias=False)\n",
       "  (embedding_o): Linear(in_features=1, out_features=256, bias=False)\n",
       "  (att_a): AlphaAttention(\n",
       "    (lin): Linear(in_features=256, out_features=1, bias=True)\n",
       "    (sparsemax): Sparsemax(dim=-1)\n",
       "    (softmax): Softmax(dim=1)\n",
       "  )\n",
       "  (att_b): BetaAttention(\n",
       "    (lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (multi_attn): MultiHeadAttention(\n",
       "    (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (fc_multi): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (conv1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "    (conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (fc): Linear(in_features=256, out_features=1071, bias=True)\n",
       "  (do): Dropout(p=0.5, inplace=False)\n",
       "  (tan): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class INPREM_O(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_codes, num_visits, embedding_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # self.embedding_v = nn.Embedding(num_codes, embedding_dim)\n",
    "        # self.embedding_o = nn.Embedding(num_codes, embedding_dim)\n",
    "        \n",
    "        self.embedding_v = nn.Linear(num_codes, embedding_dim, bias=False)\n",
    "        self.embedding_o = nn.Linear(1, embedding_dim, bias=False)\n",
    "        \n",
    "        self.att_a = AlphaAttention(embedding_dim)\n",
    "        self.att_b = BetaAttention(embedding_dim)\n",
    "        \n",
    "        self.multi_attn = MultiHeadAttention(num_codes, embedding_dim)\n",
    "        \n",
    "        self.fc = nn.Linear(embedding_dim, num_codes)\n",
    "        \n",
    "        self.do = nn.Dropout(.5)\n",
    "        \n",
    "        self.tan = nn.Tanh()\n",
    "    \n",
    "    def forward(self, X, O):\n",
    "        \"\"\"\n",
    "        Implement the INPREM Model\n",
    "        \"\"\"\n",
    "        \n",
    "        X = X.type(torch.FloatTensor)\n",
    "        \n",
    "        E = self.embedding_v(X)\n",
    "        \n",
    "        H =  self.multi_attn(E)\n",
    "        H =  self.multi_attn(H)\n",
    "        \n",
    "        beta = self.att_b(H)\n",
    "        alpha = self.att_a(H).permute(0,2,1)\n",
    "        \n",
    "        E_r = (alpha @ (beta * E))\n",
    "        \n",
    "        out = self.fc(E_r)\n",
    "        \n",
    "        out = out.squeeze()\n",
    "        out = F.softmax(out, dim=1)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "# load the model here\n",
    "model = INPREM_O(len(types), max_num_visits)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "441ceb73-afb4-427a-a619-5960198f6003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training Loss: 0.082935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 \t Training Loss: 0.081949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 \t Training Loss: 0.081131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 \t Training Loss: 0.080633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 \t Training Loss: 0.080049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model\n",
    "model = INPREM_O(len(types), max_num_visits)\n",
    "\n",
    "# load the loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# load the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "\n",
    "n_epochs = 5\n",
    "train(model, train_loader, val_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffed15d0-76a4-4604-87fa-0554df8b5c86",
   "metadata": {},
   "source": [
    "### INPREM_S (Removing Sparsemax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a0bfc516-092c-4218-9481-5766292bbf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaAttentionAlt(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lin = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, g):\n",
    "        \n",
    "        y = self.lin(g)\n",
    "        soft_max = F.softmax(y, dim=1)\n",
    "        \n",
    "        out = soft_max\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d75662a6-c6c5-4405-a64e-5ad4ee9add44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INPREM_S(\n",
       "  (embedding_v): Linear(in_features=1071, out_features=256, bias=False)\n",
       "  (embedding_o): Linear(in_features=1, out_features=256, bias=False)\n",
       "  (att_a): AlphaAttentionAlt(\n",
       "    (lin): Linear(in_features=256, out_features=1, bias=True)\n",
       "    (softmax): Softmax(dim=1)\n",
       "  )\n",
       "  (att_b): BetaAttention(\n",
       "    (lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (multi_attn): MultiHeadAttention(\n",
       "    (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (fc_multi): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (conv1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "    (conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (fc): Linear(in_features=256, out_features=1071, bias=True)\n",
       "  (do): Dropout(p=0.5, inplace=False)\n",
       "  (tan): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class INPREM_S(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_codes, num_visits, embedding_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        self.embedding_v = nn.Linear(num_codes, embedding_dim, bias=False)\n",
    "        self.embedding_o = nn.Linear(1, embedding_dim, bias=False)\n",
    "        \n",
    "        self.att_a = AlphaAttentionAlt(embedding_dim)\n",
    "        self.att_b = BetaAttention(embedding_dim)\n",
    "        \n",
    "        self.multi_attn = MultiHeadAttention(num_codes, embedding_dim)\n",
    "        \n",
    "        self.fc = nn.Linear(embedding_dim, num_codes)\n",
    "        \n",
    "        self.do = nn.Dropout(.5)\n",
    "        \n",
    "        self.tan = nn.Tanh()\n",
    "    \n",
    "    def forward(self, X, O):\n",
    "        \"\"\"\n",
    "        Implement the INPREM Model\n",
    "        \"\"\"\n",
    "        \n",
    "        X = X.type(torch.FloatTensor)\n",
    "        \n",
    "        E = self.embedding_v(X)\n",
    "        \n",
    "        H =  self.multi_attn(E)\n",
    "        H =  self.multi_attn(H)\n",
    "        \n",
    "        beta = self.att_b(H)\n",
    "        alpha = self.att_a(H).permute(0,2,1)\n",
    "        \n",
    "        E_r = (alpha @ (beta * E))\n",
    "        \n",
    "        out = self.fc(E_r)\n",
    "        \n",
    "        out = out.squeeze()\n",
    "        out = F.softmax(out, dim=1)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "# load the model here\n",
    "model = INPREM_S(len(types), max_num_visits)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5c2b2c-20df-428c-9390-b152ae67c527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training Loss: 0.082849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 \t Training Loss: 0.081906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 \t Training Loss: 0.081091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 \t Training Loss: 0.080491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "model = INPREM_S(len(types), max_num_visits)\n",
    "\n",
    "# load the loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# load the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "\n",
    "n_epochs = 5\n",
    "train(model, train_loader, val_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe5e895-e644-4790-9a3e-51d2ffdfc451",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Training and Inferencing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f978aa09",
   "metadata": {},
   "source": [
    "## 4.1 Define Evaluation Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1ee84c93-6357-4d93-b612-b7fde9d30c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "\n",
    "def eval(model, val_loader):\n",
    "    \n",
    "    \"\"\"\n",
    "    Evaluate the model.\n",
    "    \n",
    "    Arguments:\n",
    "        model: the RNN model\n",
    "        val_loader: validation dataloader\n",
    "        \n",
    "    Outputs:\n",
    "        precision: overall precision score\n",
    "        recall: overall recall score\n",
    "        f1: overall f1 score\n",
    "        roc_auc: overall roc_auc score\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    y_pred = torch.LongTensor()\n",
    "    y_score = torch.Tensor()\n",
    "    y_true = torch.LongTensor()\n",
    "    model.eval()\n",
    "    for x, o, y in val_loader:\n",
    "        y_logit = model(x, o)\n",
    "        \"\"\"\n",
    "        TODO: obtain the predicted class (0, 1) by comparing y_logit against 0.5, \n",
    "              assign the predicted class to y_hat.\n",
    "        \"\"\"\n",
    "        y_hat = (y_logit > 0.5).int()\n",
    "\n",
    "        y_score = torch.cat((y_score,  y_logit.detach().to('cpu')), dim=0)\n",
    "        y_pred = torch.cat((y_pred,  y_hat.detach().to('cpu')), dim=0)\n",
    "        y_true = torch.cat((y_true, y.detach().to('cpu')), dim=0)\n",
    "    \n",
    "    p, r, f, _ = precision_recall_fscore_support(y_true, y_pred)\n",
    "    \n",
    "    # roc_auc = roc_auc_score(y_true, y_score)\n",
    "    roc_auc = 0.0\n",
    "    \n",
    "    return p, r, f, roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e5aa54-d7b3-4983-a834-77c898657366",
   "metadata": {},
   "source": [
    "## 4.2 Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e402cdc6-bb7b-4f0b-8efd-f6efff628b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, n_epochs):\n",
    "    \"\"\"\n",
    "    Train the model.\n",
    "    \n",
    "    Arguments:\n",
    "        model: the RNN model\n",
    "        train_loader: training dataloder\n",
    "        val_loader: validation dataloader\n",
    "        n_epochs: total number of epochs\n",
    "    \n",
    "    Outputs: \n",
    "        roc_auc (rounded, 2): the ROC AUC score for the predictions\n",
    "    \"\"\"\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        train_loss = 0\n",
    "        for x, o, y in train_loader:\n",
    "        # for DATA, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            y_hat = model(x, o)\n",
    "            torch.set_printoptions(threshold=6)\n",
    "            \n",
    "            y = y.type(torch.FloatTensor)\n",
    "\n",
    "            loss = criterion(y_hat, y)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        print('Epoch: {} \\t Training Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "        \n",
    "        p, r, f, roc_auc = eval(model, val_loader)\n",
    "        \n",
    "        # print('Epoch: {} \\t Validation p: {:.2f}, r:{:.2f}, f: {:.2f}, roc_auc: {:.2f}'.format(epoch+1, p, r, f, roc_auc))\n",
    "        \n",
    "        # get performance data\n",
    "        if print_metrics:\n",
    "            print('The CPU usage over the last 5 seconds is: {}'.format(psutil.cpu_percent(5)))\n",
    "            print('RAM used is: {} GB'.format(psutil.virtual_memory()[3]/1000000000))\n",
    "            print('GPU memory (VRAM) used: {} GB'.format((torch.cuda.mem_get_info()[1] - torch.cuda.mem_get_info()[0])/1000000000))\n",
    "        \n",
    "    return round(roc_auc, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630845b1-f35a-4f28-93d3-593bfa3e9993",
   "metadata": {},
   "source": [
    "### Run Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35875830-7833-4907-9b0e-721957ac0db9",
   "metadata": {},
   "source": [
    "For training all approaches, we use Adam with the batch size of 32 and the learning rate of 0.0005. The weight decay is set to 𝜆 = 0.0001 and the dropout rate is set to 0.5 for all approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3ffaa109-7d45-4a3e-b9bb-0c9748a12770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([ True, False, False,  ..., False,  True, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([ True, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False,  True])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False,  True])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "Epoch: 1 \t Training Loss: 0.359537\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([ True, False, False,  ..., False,  True, False])\n",
      "tensor([ True, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False,  True])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False,  True])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "Epoch: 2 \t Training Loss: 0.357324\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False,  True])\n",
      "tensor([False, False, False,  ..., False, False,  True])\n",
      "tensor([ True, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([ True, False, False,  ..., False,  True, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "Epoch: 3 \t Training Loss: 0.355351\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([ True, False, False,  ..., False,  True, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False,  True])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([ True, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False,  True])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "Epoch: 4 \t Training Loss: 0.353543\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([ True, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False,  True])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([ True, False, False,  ..., False,  True, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False,  True])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "Epoch: 5 \t Training Loss: 0.351835\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "tensor([False, False, False,  ..., False, False, False])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model\n",
    "model = INPREM(len(types), max_num_visits)\n",
    "\n",
    "# load the loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# load the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "\n",
    "n_epochs = 5\n",
    "train(model, train_loader, val_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3b9314-e48e-41f1-bfec-a23381f59b6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7bbc121b-8d7d-4d90-84b8-eed2a485ace3",
   "metadata": {},
   "source": [
    "## Abblations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c088435-9f4e-47e4-9fd2-3743022347db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa0f9c2-bc37-46de-bc94-36feba5fea66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7aa2f7-8c59-4eb2-b373-05eb7da9729b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
